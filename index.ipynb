{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This short lesson summarizes key takeaways from section 33."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "You will be able to:\n",
    "* Understand and explain what was covered in this section\n",
    "* Understand and explain why this section will help you become a data scientist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "The key takeaways from this section include:\n",
    "* Support Vector Machines can be used for regression tasks, although they're better known as a powerful algorithm for classification.\n",
    "* SVMs optimize for maximizing the margin between the decision boundary and the nearest data points (the support vectors)\n",
    "* For data that isn't linearly seperable (no single straight line will correctly classify all of the observations into the correct categpries) a soft margin classifier can be used to allow for a model that may mis-classify some of the training data points\n",
    "* You can improve the performance of your SVMs for data sets where a linear decision boundary isn't very useful by using the kernel trick\n",
    "* With the kernel trick, you project your data set into a higher dimensional space \n",
    "* The Gaussian/Radial Basis Function (RBF) kernel provides you with tro hyper parameters - C and Gamma\n",
    "* C allows you to trade off between misclassification of the training set and simplicity of the decision function (to avoid overfitting). It's common to all kernels\n",
    "* Gamma allows you to define how much influence a single training example has\n",
    "* The polynomial kernel is specified as $$(\\gamma \\langle  x -  x' \\rangle+r)^d $$\n",
    "* The sigmoid kernel is specified as $$\\tanh ( \\gamma\\langle  x -  x' \\rangle+r) $$\n",
    "* Based on the \"no free lunch\" theorum, there's no kernel that is guaranteed to be better for a given training set than others, but the RBF kernel is often a good default to start with\n",
    "* in Scikit-learn, the SVC method is one classifier supporting multiple kernels. It takes C as the penalty parameter and additional kernel specific parameters such as degree (for polynomail) and gamma (for RBF, polynomial and sigmoid) https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "* There are other implementations of SVM built into Scikit-learn. For example:\n",
    "    * NuSVC introduces $\\nu$ (pronounced \"nu\") creates an upper bound on training errors and a lower bound on support vectors\n",
    "    * LinearSVC is a \"one-vs-rest\" method which often scales better for cases with many potential classes \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
